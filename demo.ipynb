{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dacl-demo\n",
    "\n",
    "In this demo-notebook you will be importing a dacl-model for the **multi-target classification** of defects on reinforced concrete structures. \n",
    "All steps for feeding the dacl model with tasty images of damage will be examined as follows:\n",
    "\n",
    "1. ***Create the DaclNet-class:*** For being able to instantiate a model, its class, named DaclNet, has to be created. Currently, there are three architectures available based on: ResNet50, EfficientNetV1-B0 and MobileNetV3-large.\n",
    "2. ***Preprocess the Image***: Before feeding the dacl-model, you need to prepare the image. The dacl models are very picky regarding their food.\n",
    "3. ***Feed the dacl-model***: Finally, feed the dacl-model with tasty preprocessed image data.\n",
    "4. ***Analyze the Test Results***: Analyze the dacl-model's results and try to interpret them. You will also see how we calculate our metrics.\n",
    "5. ***Export Results to CSV-file***: You might want to save your test results for submitting to dacl.ai.\n",
    "\n",
    "**HAVE FUN!!!**\n",
    "\n",
    "<img src='https://dacl.ai/assets/DACL_pixel.png' width=300px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Initially, you have to import the modules you want to use. Make sure that you have installed all prerequisites according to the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, json, random \n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection, F1Score\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import MemoryEfficientSwish\n",
    "\n",
    "from bikit.utils import list_datasets, download_dataset \n",
    "from bikit.datasets import BikitDataset \n",
    "from bikit.metrics import EMR_mt, Recalls_mt\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the DaclNet-class\n",
    "\n",
    "First, you need to define the model with the DaclNet class. Currently, there are three architectures available based on: ResNet 50, EfficientNetV1-B0 and MobileNetV3-Large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to find the suiting EfficientNet model according to the resolution of the input-images:\n",
    "efnet_dict = {'b0': 224, 'b1': 240, 'b2': 260, 'b3': 300,   \n",
    "              'b4': 380, 'b5': 456, 'b6': 528, 'b7': 600    \n",
    "             }\n",
    "\n",
    "class DaclNet(nn.Module):\n",
    "    def __init__(self, base_name, resolution, hidden_layers, num_class, drop_prob=0.2, freeze_base=True):\n",
    "        ''' \n",
    "        Builds a network separated into a base model and classifier with arbitrary hidden layers.\n",
    "        \n",
    "        Attributes\n",
    "        ---------\n",
    "        base_name:      string, basemodel for the NN\n",
    "        resolution:     resolution of the input-images, example: 224, 240...(look efnet_dic), Only needed for EfficientNet\n",
    "        hidden_layers:  list of integers, the sizes of the hidden layers\n",
    "        drop_prob:      float, dropout probability\n",
    "        freeze_base:    boolean, choose if you want to freeze the parameters of the base model\n",
    "        num_class:      integer, size of the output layer according to the number of classes\n",
    "\n",
    "        Example\n",
    "        ---------\n",
    "        model = Network(base_name='efficientnet', resolution=224, hidden_layers=[32,16], num_class=6, drop_prob=0.2, freeze_base=True)\n",
    "        \n",
    "        Note\n",
    "        ---------\n",
    "        -print(efficientnet) -> Last module: (_swish): MemoryEfficientSwish() and the last fc-layers are displayed\n",
    "         This activation won't be called during forward due to: \"self.base.extract_features\"! No activation of last layer!\n",
    "        '''\n",
    "        super(DaclNet, self).__init__()\n",
    "        # basemodel\n",
    "        self.base_name = base_name\n",
    "        self.resolution = resolution\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.freeze_base = freeze_base\n",
    "\n",
    "        if self.base_name == 'mobilenet':\n",
    "            base = models.mobilenet_v3_large(pretrained=True) \n",
    "            modules = list(base.children())[:-1] \n",
    "            self.base = nn.Sequential(*modules)\n",
    "            # for pytorch model:\n",
    "            if hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[0].in_features, self.hidden_layers[0])]) \n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.classifier[0].in_features, num_class)\n",
    "\n",
    "            self.activation = nn.Hardswish()\n",
    "\n",
    "        elif self.base_name == 'resnet':\n",
    "            base = models.resnet50(pretrained=True) \n",
    "            modules = list(base.children())[:-1]\n",
    "            self.base = nn.Sequential(*modules)\n",
    "            if self.hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.fc.in_features, self.hidden_layers[0])])\n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.fc.in_features, num_class)   \n",
    "            self.activation = nn.ELU() \n",
    "\n",
    "        elif self.base_name == 'efficientnet':      \n",
    "            for ver in efnet_dict:\n",
    "                if efnet_dict[ver] == self.resolution:\n",
    "                    self.version = ver\n",
    "                    full_name = self.base_name+'-'+ver\n",
    "            self.base = EfficientNet.from_pretrained(model_name=full_name) \n",
    "            if self.hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(self.base._fc.in_features, self.hidden_layers[0])])\n",
    "            else:\n",
    "                self.classifier = nn.Linear(self.base._fc.in_features, num_class)   \n",
    "            self.activation = MemoryEfficientSwish()\n",
    "            \n",
    "        elif self.base_name == 'mobilenetv2':\n",
    "            base = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "            modules = list(base.children())[:-1]\n",
    "            self.base = nn.Sequential(*modules)\n",
    "            if hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[1].in_features, self.hidden_layers[0])]) \n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.classifier[1].in_features, num_class)\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise NotImplementedError    \n",
    "        \n",
    "        # freeze the base\n",
    "        if self.freeze_base:\n",
    "            for param in self.base.parameters(): \n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob, inplace=True)\n",
    "\n",
    "        # classifier\n",
    "        # Add a variable number of more hidden layers\n",
    "        if self.hidden_layers:\n",
    "            layer_sizes = zip(self.hidden_layers[:-1], self.hidden_layers[1:])        \n",
    "            self.classifier.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "            # Add output layer to classifier\n",
    "            self.classifier.append(nn.Linear(self.hidden_layers[-1], num_class))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        ''' \n",
    "        Performs the feed-forward process for the input batch and returns the logits\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        input_batch: torch.Tensor, Multidimensional array holding elements of datatype: torch.float32, \n",
    "                     it's shape is: [1, 3, 224, 224] according to N x C x H x W,\n",
    "                     The input batch carries all pixel values from the images inside teh batch\n",
    "        Note\n",
    "        ---------\n",
    "        Every model uses 2d-Average-Pooling with output_size=1 after the feature extraction or rather before flattening.\n",
    "        The pooling layer of ResNet50 and MobileNetV3 was kept in the squential -> Doesn't have to be called in forward!\n",
    "        EffNet had to be implemented with the AdaptiveAvgpool2d in this forward function because of missing pooling when\n",
    "        calling: \"effnet.extract_features(input_batch)\"\n",
    "        Also mobilenetV2 needs the manually added pooling layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        logits: torch.Tensor, shape: [1, num_class], datatype of elements: float\n",
    "        '''\n",
    "        # Check if model is one that needs Pooling layer and/or special feature extraction\n",
    "        if self.base_name in ['efficientnet', 'mobilenetv2']:\n",
    "            if self.base_name == 'efficientnet':\n",
    "                x = self.base.extract_features(input_batch)\n",
    "            else:\n",
    "                # For MobileNetV2\n",
    "                x= self.base(input_batch)\n",
    "            pool = nn.AdaptiveAvgPool2d(1)\n",
    "            x = pool(x)\n",
    "        else:\n",
    "            # For any other model don't additionally apply pooling:\n",
    "            x = self.base(input_batch)\n",
    "        \n",
    "        x = self.dropout(x)         # Originally only in EfficientNet a Dropout after feature extraction is added  \n",
    "        x = x.view(x.size(0), -1)   # Or: x.flatten(start_dim=1)\n",
    "        if self.hidden_layers:    \n",
    "            for i,each in enumerate(self.classifier):\n",
    "                # Put an activation function and dropout after each hidden layer\n",
    "                if i < len(self.classifier)-1:\n",
    "                    x = self.activation(each(x))\n",
    "                    x = self.dropout(x)\n",
    "                else:\n",
    "                    # Don't use an activation and dropout for the last layer\n",
    "                    logits = each(x)\n",
    "                    break\n",
    "        else:\n",
    "            logits = self.classifier(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Helper Functions\n",
    "\n",
    "Before feeding the dacl-model, you need to prepare the image. The dacl models are very picky regarding their food.\n",
    "You want to get an image transformed to a tensor with the shape *N x C x H x W* where *N* is the batch-size, *C* the color channels (RGB), *H* the height and *W* the width of the image. Also, there's a function for selecting an arbitrary image and one to display our result next to the classified image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing-functions:\n",
    "def process_img(img_path=None):\n",
    "\t''' \n",
    "\tScales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "\treturns a Torch Tensor\n",
    "\tArgs: \n",
    "\t\timg_path: \tstring, filepath of the image\n",
    "\tExample: \n",
    "\t\tprocess_img('test/1/image_06743.jpg')\n",
    "\tReturns: \n",
    "\t\ttorch.float32 of shape: [1, 3, 224, 224]\n",
    "\t'''\n",
    "\n",
    "\tif not img_path:\n",
    "\t\tprint('Parse the filename of the image!')\n",
    "\telse:\n",
    "\t\t#Parse image as PIL Image\n",
    "\t\timage = Image.open(img_path)\n",
    "\t\t# Setting Resize Parameters (width and height)\n",
    "\t\timage_ratio = image.height / image.width\n",
    "\t\tif  image.width < image.height  or image.width > image.height:\n",
    "\t\t\tif image.width < image.height:\n",
    "\t\t\t\tresize = (256, int(image_ratio * 256))\n",
    "\t\t\telse:\n",
    "\t\t\t\tresize = (int(256 / image_ratio), 256)\n",
    "\t\telse:\n",
    "\t\t\tresize = (256, 256)\n",
    "\t\t\n",
    "\t\t#Setting Crop parameters\n",
    "\t\tcrop_size = 224\n",
    "\t\tcrop_x = int((resize[0] - crop_size) / 2)\n",
    "\t\tcrop_y = int((resize[1] - crop_size) / 2)\n",
    "\t\tcrop_box = (crop_x, crop_y,crop_x + crop_size, crop_y+crop_size)\n",
    "\t  \t\n",
    "\t\t#Transformation\n",
    "\t\tpil_image = image.resize(resize)\n",
    "\t\tpil_image = pil_image.crop(crop_box)\n",
    "\t\tnp_image = np.array(pil_image)\n",
    "\t\tnp_image = (np_image/255 - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "\t\tnp_image = np_image.transpose(2,0,1)\n",
    "\t\timage = torch.from_numpy(np_image)\n",
    "\t\timage = image.unsqueeze_(0)\n",
    "\t\timage = image.type(torch.FloatTensor)\n",
    "\t\treturn image\n",
    "\n",
    "def rand_img_path(dir):\n",
    "\t''' \n",
    "\tReturns an arbitrary path to an image file from a directory, also in its subfolders.\n",
    "\tArgs: \n",
    "\t\tdir: string, path to the directory including the images (string)\n",
    "\tExample: \n",
    "\t\trand_img_path('assets/DamageExamples')\n",
    "\t'''\n",
    "\tfile = os.path.join(dir, random.choice(os.listdir(dir)));\n",
    "\tif os.path.isdir(file):\n",
    "\t\treturn rand_img_path(file)\n",
    "\telse:\n",
    "\t\treturn file\n",
    "\n",
    "def view_classify(img_path, result_dict):\n",
    "\t''' \n",
    "\tFunction for viewing an image, its predicted classes and the probabilities\n",
    "  \tin a horizontal bar chart.\n",
    "  \tArgs:\n",
    "  \t\timage_path:\t\tstring, path to image you want to classify. You can take random_image_path \n",
    "\t\t  \t\t\t\tfunction so a random image from test-folder will be classified\n",
    "  \t\tresult_dict:\tdict, result_dict returned by the predict function\n",
    "  \tReturns:\n",
    "  \t\tNone - just displays the image next to the bar chart  \n",
    "  \t'''\n",
    "\tresult_list = list(result_dict.items())\n",
    "\tresult_list = sorted(result_list, reverse=False, key=lambda result: result[1])\n",
    "\tcat_names = [x[0] for x in result_list]\n",
    "\tps = [x[1] for x in result_list]\t\n",
    "\tfig, (ax1, ax2) = plt.subplots(figsize=(9,12), ncols=2)\n",
    "\tax1.imshow(plt.imread(img_path))\n",
    "\tax1.axis('off')\n",
    "\t\n",
    "\t# create title:\n",
    "\ttitle = result_list[-1][0]\n",
    "\tfor i in range((len(result_list)-2), 0, -1):\n",
    "\t\tif result_list[i][1] > .5:\n",
    "\t\t\ttitle += (', ' + result_list[i][0])\n",
    "\tax1.set_title(title)\n",
    "\n",
    "\tax2.barh(range(len(cat_names)), ps, align='center')\n",
    "\tax2.set_aspect(0.1)\n",
    "\tax2.set_yticks(np.arange(len(cat_names)))\n",
    "\tax2.set_yticklabels(cat_names, size='small')\n",
    "\tax2.set_title('Class Probability')\n",
    "\tax2.set_xlim(0, 1.1)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\n",
    "\n",
    "# Get image and show it:\n",
    "img_dir = 'assets/DamageExamples'\n",
    "img_path = rand_img_path(img_dir)\n",
    "try:\n",
    "\timg = plt.imread(img_path)\n",
    "except:\n",
    "\tprint(\"ERROR: {} is no image file. Don't leave non-image files in the {} folder or change the img_dir!\".format(img_path, img_dir))\n",
    "plt.imshow(img)\n",
    "\n",
    "# Preprocess:\n",
    "img_proc = process_img(img_path)\n",
    "img_proc = img_proc.to(device)\n",
    "print(\"The datatype of the preprocessed image is: {} and its shape: {}\".format(img_proc.dtype, img_proc.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feed the dacl-model\n",
    "Finally, you can instantiate the model, load a checkpoint of your choice and feed the dacl-model with tasty preprocessed image data. You will see the predicted result under the following code cell.\n",
    "\n",
    "**Choose a checkpoint from the table below**\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Modelname             | Dataset           | EMR   | F1   | Tag          | Checkpoint                |\n",
    "|-----------------------|-------------------|-------|------|--------------|---------------------------|\n",
    "| Code_res_dacl         | codebrim_balanced | 73.73 | 0.85 | ResNet       | Code_res_dacl.pth         |\n",
    "| Code_mobilev2_dacl    | codebrim_balanced |70.41  | 0.84 | MobileNetV2  | Code_mobilev2_dacl.pth    |\n",
    "| Code_mobile_dacl      | codebrim_balanced | 69.46 | 0.83 | MobileNet    | Code_mobile_dacl.pth      |\n",
    "| Code_eff_dacl         | codebrim_balanced | 68.67 | 0.84 | EfficientNet | Code_eff_dacl.pth         |\n",
    "| McdsBikit_mobile_dacl | mcds_Bikit        | 54.44 | 0.66 | MobileNet    | McdsBikit_mobile_dacl.pth |\n",
    "| McdsBikit_eff_dacl    | mcds_Bikit        | 51.85 | 0.65 | EfficientNet | McdsBikit_eff_dacl.pth    |\n",
    "| McdsBikit_res_dacl    | mcds_Bikit        | 48.15 | 0.62 | ResNet       | McdsBikit_res_dacl.pth    |\n",
    "\n",
    "** *All these models are available via **bikit**. Check out this repo's README for further information!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which checkpoint/model you want to load from the table above:\n",
    "cp_name = 'Code_res_dacl.pth'\n",
    "\n",
    "# Load the checkpoint:\n",
    "cp = torch.load(Path('models/' + cp_name)) \n",
    "\n",
    "# Instantiate the model:\n",
    "model = DaclNet(base_name=cp['base'], resolution = cp['resolution'], hidden_layers=cp['hidden_layers'], \n",
    "\t\t\t\tdrop_prob=cp['drop_prob'], num_class=cp['num_class'])\n",
    "model.load_state_dict(cp['state_dict']) # Load the pre-trained weights into the model\n",
    "model.eval() # Set the model to eval-mode. No dropout and no autograd will be applied.\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad(): # Disable tracking of gradients in autograd (saves some time)\n",
    "    \n",
    "    # Now, let's feed the dacl-model in order to  classify the preprocessed image that we imported at the beginning:\n",
    "    logits = model(img_proc)\n",
    "\n",
    "    # Apply sigmoid activation to get predictions:\n",
    "    preds = torch.sigmoid(logits).float().squeeze(0).to('cpu')\n",
    "\n",
    "# Binarize results:\n",
    "threshold = .5 # Which threshold do you want to choose for binarization of predictions? (for bikit, 0.5 was chosen)\n",
    "bin = np.array(preds > threshold, dtype=float)\n",
    "\n",
    "# In the cat_to_name file our damage-class-names are stored with the according position in the output vector:\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "\tcat_to_name = json.load(f)[cp['dataset']]\n",
    "\n",
    "# Output:\n",
    "# Make a dict with the predictions:\n",
    "preds_dict = {v:round(preds[int(k)].item(),2) for k,v in cat_to_name.items()}\n",
    "print('*'*10, 'Output', '*'*10)\n",
    "\n",
    "for k,v in preds_dict.items():\n",
    "\tif v > .5:\n",
    "\t\tprint('%s: %.2f%%' % (k,v*100)) \n",
    "\n",
    "# View the classified image and it's predictions:\n",
    "view_classify(img_path, preds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze the Test Results\n",
    "Analyze the dacl-model's test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test transforms:\n",
    "test_transforms = transforms.Compose([transforms.Resize(int(1.1*cp['resolution'])),\n",
    "                                      transforms.CenterCrop(cp['resolution']),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Download dataset:\n",
    "download_dataset(cp['dataset']) \n",
    "\n",
    "# Instantiate test-dataset and -loader from bikit:\n",
    "test_dataset = BikitDataset(name=cp['dataset'], split=\"test\", transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(\"======test_dataset======\\n\", test_dataset.df[test_dataset.class_names].sum())\n",
    "\n",
    "# Pack all metrics you want to calculate inside one MetricCollection from torchmetrics:\n",
    "metrics = MetricCollection([EMR_mt(use_logits=False),\n",
    "                            F1Score(num_classes=test_dataset.num_classes, average='macro', compute_on_step=False),\n",
    "                            Recalls_mt(num_classes=test_dataset.num_classes)]).to(device) # classwise Recall\n",
    "\n",
    "# Define your loss (Optional):\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "sum_counts = 0\n",
    "cumu_loss = 0\n",
    "\n",
    "start = time.time() # Save the starting time\n",
    "\n",
    "# Start the test loop:\n",
    "with torch.no_grad():\n",
    "    for i, (data,targets) in enumerate(test_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        logits = model(data)\n",
    "        preds = torch.sigmoid(logits).float()\n",
    "        loss = criterion(logits, targets)     \n",
    "        bs = targets.shape[0]\n",
    "        sum_counts += bs\n",
    "        cumu_loss += loss.item() * bs\n",
    "\n",
    "        metrics(preds, targets.int())\n",
    "\n",
    "        cumu_preds = preds if i == 0 else torch.cat((cumu_preds, preds), 0)\n",
    "        cumu_targets = targets if i == 0 else torch.cat((cumu_targets, targets), 0)\n",
    "        \n",
    "    total_loss = cumu_loss/sum_counts\n",
    "    metrics = metrics.compute() # Compute the metrics after having finnished the test loop\n",
    "\n",
    "\n",
    "# Print all metrics you are curious about:    \n",
    "print('\\n======Finnished Testing======')\n",
    "print('Tested dataset:    %s'      % cp['dataset'])\n",
    "print(\"Dacl's base-arch:  %s\"      % cp['base'])\n",
    "print(\"TL-approach:       %s\"      % cp['approach'])\n",
    "print('Test Loss:         %.4f'    % total_loss)\n",
    "print('ExactMatchRatio:   %.2f %%' % (metrics[\"EMR_mt\"].item()*100))\n",
    "print('F1-Score:          %.2f'    % metrics[\"F1Score\"].item())\n",
    "print('Time fore testing: %d s\\n'  % (time.time() - start))\n",
    "for c in cat_to_name:\n",
    "    print('Recall-%s: %.2f' % (cat_to_name[c], metrics['Recalls_mt'][int(c)]) )\n",
    "\n",
    "    \n",
    "    \n",
    "# Get the amount of completely correct predictions (Numerator in exact match ratio, EMR):\n",
    "correct = 0\n",
    "y_hat = (cumu_preds > threshold) # Return True for each item in cumu_preds above threshold\n",
    "z_match = (y_hat == cumu_targets) # Return True for each item matching the corresponding one in the targets tensor\n",
    "z = torch.all(z_match, dim=1) # Check across the first dimension (width of the Tensor) if all items are True. If so return True for that sample.\n",
    "for i in z:\n",
    "    if True in z[i]:\n",
    "        # Count all exact matches:\n",
    "        correct += 1\n",
    "    else:\n",
    "        pass\n",
    "print('\\nCompletely correct predicted samples: %s' % correct) \n",
    "print('Number of Samples in test dataset:    %s'   % len(test_dataset))\n",
    "\n",
    "# Now we can calculate the 'by hand'-EMR:\n",
    "byhand_emr = correct/cumu_targets.shape[0] \n",
    "print(\"ExactMatchRatio calculated 'by hand': %.2f %%\" % (byhand_emr * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to CSV-file \n",
    "For submitting your results to dacl.ai you have to send us your predictions on the test data. You can do so by the following code.\n",
    "Attention: Don't shuffle your batches if you send us your results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_np = y_hat.to('cpu').numpy()\n",
    "y_hat_df = pd.DataFrame(y_hat_np)\n",
    "y_hat_df.to_csv('y_hat_{}_{}.csv'.format(cp['dataset'], cp['base']))\n",
    "\n",
    "cumu_targets_np = cumu_targets.to('cpu').numpy()\n",
    "cumu_targets_df = pd.DataFrame(cumu_targets_np)\n",
    "cumu_targets_df.to_csv('cumu_targets_{}_{}.csv'.format(cp['dataset'], cp['base']))\n",
    "\n",
    "z_np = z.to('cpu').numpy()\n",
    "z_df = pd.DataFrame(z_np)\n",
    "z_df.to_csv('z_{}_{}.csv'.format(cp['dataset'], cp['base']))\n",
    "\n",
    "z_match_np = z_match.to('cpu').numpy()\n",
    "z_match_df = pd.DataFrame(z_match_np)\n",
    "z_match_df.to_csv('z_match_{}_{}.csv'.format(cp['dataset'], cp['base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef6e183be3d057400a617fb7203f5cb36fc2e2b6cae49034cff8a52636ae983"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
