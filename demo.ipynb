{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dacl-demo\n",
    "\n",
    "In this demo-notebook you will be importing a dacl-model for the **multi-target classification** of defects on reinforced concrete structures. \n",
    "All steps for feeding the dacl model with tasty images of damage will be examined as follows:\n",
    "\n",
    "1. ***Create the DaclNet-class:*** For being able to instantiate a model, its class, named DaclNet, has to be created. Currently, there are three architectures available based on: ResNet50, EfficientNetV1-B0 and MobileNetV3-large.\n",
    "2. ***Preprocess the image***: Before feeding the dacl-model, you need to prepare the image. The dacl models are very picky regarding their food.\n",
    "3. ***Feed the dacl-model***: Finally, feed the dacl-model with tasty preprocessed image data.\n",
    "4. ***Analyze the results***: Analyze the dacl-model's results and try to interpret them.\n",
    "\n",
    "**HAVE FUN!!!**\n",
    "\n",
    "<img src='https://dacl.ai/assets/DACL_pixel.png' width=300px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Initially, you have to import the modules you want to use. Make sure that you have installed all prerequisites according to the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import MemoryEfficientSwish\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the DaclNet-class\n",
    "\n",
    "First, you need to define the model with the DaclNet class. Currently, there are three architectures available based on: ResNet 50, EfficientNetV1-B0 and MobileNetV3-Large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to find the suiting EfficientNet model according to the resolution of the input-images:\n",
    "efnet_dict = {'b0': 224, 'b1': 240, 'b2': 260, 'b3': 300,   \n",
    "              'b4': 380, 'b5': 456, 'b6': 528, 'b7': 600    \n",
    "             }\n",
    "\n",
    "class DaclNet(nn.Module):\n",
    "    def __init__(self, base_name, resolution, hidden_layers, num_class, drop_prob=0.2, freeze_base=True):\n",
    "        ''' \n",
    "        Builds a network separated into a base model and classifier with arbitrary hidden layers.\n",
    "        \n",
    "        Attributes\n",
    "        ---------\n",
    "        base_name:      string, basemodel for the NN\n",
    "        resolution:     resolution of the input-images, example: 224, 240...(look efnet_dic), Only needed for EfficientNet\n",
    "        hidden_layers:  list of integers, the sizes of the hidden layers\n",
    "        drop_prob:      float, dropout probability\n",
    "        freeze_base:    boolean, choose if you want to freeze the parameters of the base model\n",
    "        num_class:      integer, size of the output layer according to the number of classes\n",
    "\n",
    "        Example\n",
    "        ---------\n",
    "        model = Network(base_name='efficientnet', resolution=224, hidden_layers=[32,16], num_class=6, drop_prob=0.2, freeze_base=True)\n",
    "        \n",
    "        Note\n",
    "        ---------\n",
    "        -print(efficientnet) -> Last module: (_swish): MemoryEfficientSwish() and the last fc-layers are displayed\n",
    "         This activation won't be called during forward due to: \"self.base.extract_features\"! No activation of last layer!\n",
    "        '''\n",
    "        super(DaclNet, self).__init__()\n",
    "        # basemodel\n",
    "        self.base_name = base_name\n",
    "        self.resolution = resolution\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.freeze_base = freeze_base\n",
    "\n",
    "        if self.base_name == 'mobilenet':\n",
    "            base = models.mobilenet_v3_large(pretrained=True) \n",
    "            modules = list(base.children())[:-1] \n",
    "            self.base = nn.Sequential(*modules)\n",
    "            # for pytorch model:\n",
    "            if hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[0].in_features, self.hidden_layers[0])]) \n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.classifier[0].in_features, num_class)\n",
    "\n",
    "            self.activation = nn.Hardswish()\n",
    "\n",
    "        elif self.base_name == 'resnet':\n",
    "            base = models.resnet50(pretrained=True) \n",
    "            modules = list(base.children())[:-1]\n",
    "            self.base = nn.Sequential(*modules)\n",
    "            if self.hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.fc.in_features, self.hidden_layers[0])])\n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.fc.in_features, num_class)   \n",
    "            self.activation = nn.ELU() \n",
    "\n",
    "        elif self.base_name == 'efficientnet':      \n",
    "            for ver in efnet_dict:\n",
    "                if efnet_dict[ver] == self.resolution:\n",
    "                    self.version = ver\n",
    "                    full_name = self.base_name+'-'+ver\n",
    "            self.base = EfficientNet.from_pretrained(model_name=full_name) \n",
    "            if self.hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(self.base._fc.in_features, self.hidden_layers[0])])\n",
    "            else:\n",
    "                self.classifier = nn.Linear(self.base._fc.in_features, num_class)   \n",
    "            self.activation = MemoryEfficientSwish()\n",
    "            \n",
    "        elif self.base_name == 'mobilenetv2':\n",
    "            base = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "            modules = list(base.children())[:-1]\n",
    "            self.base = nn.Sequential(*modules)\n",
    "            if hidden_layers:\n",
    "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[1].in_features, self.hidden_layers[0])]) \n",
    "            else:\n",
    "                self.classifier = nn.Linear(base.classifier[1].in_features, num_class)\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise NotImplementedError    \n",
    "        \n",
    "        # freeze the base\n",
    "        if self.freeze_base:\n",
    "            for param in self.base.parameters(): \n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob, inplace=True)\n",
    "\n",
    "        # classifier\n",
    "        # Add a variable number of more hidden layers\n",
    "        if self.hidden_layers:\n",
    "            layer_sizes = zip(self.hidden_layers[:-1], self.hidden_layers[1:])        \n",
    "            self.classifier.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "            # Add output layer to classifier\n",
    "            self.classifier.append(nn.Linear(self.hidden_layers[-1], num_class))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        ''' \n",
    "        Performs the feed-forward process for the input batch and returns the logits\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        input_batch: torch.Tensor, Multidimensional array holding elements of datatype: torch.float32, \n",
    "                     it's shape is: [1, 3, 224, 224] according to N x C x H x W,\n",
    "                     The input batch carries all pixel values from the images inside teh batch\n",
    "        Note\n",
    "        ---------\n",
    "        Every model uses 2d-Average-Pooling with output_size=1 after the feature extraction or rather before flattening.\n",
    "        The pooling layer of ResNet50 and MobileNetV3 was kept in the squential -> Doesn't have to be called in forward!\n",
    "        EffNet had to be implemented with the AdaptiveAvgpool2d in this forward function because of missing pooling when\n",
    "        calling: \"effnet.extract_features(input_batch)\"\n",
    "        Also mobilenetV2 needs the manually added pooling layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        logits: torch.Tensor, shape: [1, num_class], datatype of elements: float\n",
    "        '''\n",
    "        # Check if model is one that needs Pooling layer and/or special feature extraction\n",
    "        if self.base_name in ['efficientnet', 'mobilenetv2']:\n",
    "            if self.base_name == 'efficientnet':\n",
    "                x = self.base.extract_features(input_batch)\n",
    "            else:\n",
    "                # For MobileNetV2\n",
    "                x= self.base(input_batch)\n",
    "            pool = nn.AdaptiveAvgPool2d(1)\n",
    "            x = pool(x)\n",
    "        else:\n",
    "            # For any other model don't additionally apply pooling:\n",
    "            x = self.base(input_batch)\n",
    "        \n",
    "        x = self.dropout(x)         # Originally only in EfficientNet a Dropout after feature extraction is added  \n",
    "        x = x.view(x.size(0), -1)   # Or: x.flatten(start_dim=1)\n",
    "        if self.hidden_layers:    \n",
    "            for i,each in enumerate(self.classifier):\n",
    "                # Put an activation function and dropout after each hidden layer\n",
    "                if i < len(self.classifier)-1:\n",
    "                    x = self.activation(each(x))\n",
    "                    x = self.dropout(x)\n",
    "                else:\n",
    "                    # Don't use an activation and dropout for the last layer\n",
    "                    logits = each(x)\n",
    "                    break\n",
    "        else:\n",
    "            logits = self.classifier(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and helper functions\n",
    "\n",
    "Before feeding the dacl-model, you need to prepare the image. The dacl models are very picky regarding their food.\n",
    "You want to get an image transformed to a tensor with the shape *N x C x H x W* where *N* is the batch-size, *C* the color channels (RGB), *H* the height and *W* the width of the image. Also, there's a function for selecting an arbitrary image and one to display our result next to the classified image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing-function:\n",
    "def process_img(img_path=None):\n",
    "\t''' \n",
    "\tScales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "\treturns a Torch Tensor\n",
    "\tArgs: \n",
    "\t\timg_path: \tstring, filepath of the image\n",
    "\tExample: \n",
    "\t\tprocess_img('test/1/image_06743.jpg')\n",
    "\tReturns: \n",
    "\t\ttorch.float32 of shape: [1, 3, 224, 224]\n",
    "\t'''\n",
    "\n",
    "\tif not img_path:\n",
    "\t\tprint('Parse the filename of the image!')\n",
    "\telse:\n",
    "\t\t#Parse image as PIL Image\n",
    "\t\timage = Image.open(img_path)\n",
    "\t\t# Setting Resize Parameters (width and height)\n",
    "\t\timage_ratio = image.height / image.width\n",
    "\t\tif  image.width < image.height  or image.width > image.height:\n",
    "\t\t\tif image.width < image.height:\n",
    "\t\t\t\tresize = (256, int(image_ratio * 256))\n",
    "\t\t\telse:\n",
    "\t\t\t\tresize = (int(256 / image_ratio), 256)\n",
    "\t\telse:\n",
    "\t\t\tresize = (256, 256)\n",
    "\t\t\n",
    "\t\t#Setting Crop parameters\n",
    "\t\tcrop_size = 224\n",
    "\t\tcrop_x = int((resize[0] - crop_size) / 2)\n",
    "\t\tcrop_y = int((resize[1] - crop_size) / 2)\n",
    "\t\tcrop_box = (crop_x, crop_y,crop_x + crop_size, crop_y+crop_size)\n",
    "\t  \t\n",
    "\t\t#Transformation\n",
    "\t\tpil_image = image.resize(resize)\n",
    "\t\tpil_image = pil_image.crop(crop_box)\n",
    "\t\tnp_image = np.array(pil_image)\n",
    "\t\tnp_image = (np_image/255 - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "\t\tnp_image = np_image.transpose(2,0,1)\n",
    "\t\timage = torch.from_numpy(np_image)\n",
    "\t\timage = image.unsqueeze_(0)\n",
    "\t\timage = image.type(torch.FloatTensor)\n",
    "\t\treturn image\n",
    "\n",
    "def rand_img_path(dir):\n",
    "\t''' \n",
    "\tReturns an arbitrary path to an image file from a directory, also in its subfolders.\n",
    "\tArgs: \n",
    "\t\tdir: string, path to the directory including the images (string)\n",
    "\tExample: \n",
    "\t\trand_img_path('assets/DamageExamples')\n",
    "\t'''\n",
    "\tfile = os.path.join(dir, random.choice(os.listdir(dir)));\n",
    "\tif os.path.isdir(file):\n",
    "\t\treturn rand_img_path(file)\n",
    "\telse:\n",
    "\t\treturn file\n",
    "\n",
    "def view_classify(img_path, result_dict):\n",
    "\t''' \n",
    "\tFunction for viewing an image, its predicted classes and the probabilities\n",
    "  \tin a horizontal bar chart.\n",
    "  \tArgs:\n",
    "  \t\timage_path:\t\tstring, path to image you want to classify. You can take random_image_path \n",
    "\t\t  \t\t\t\tfunction so a random image from test-folder will be classified\n",
    "  \t\tresult_dict:\tdict, result_dict returned by the predict function\n",
    "  \tReturns:\n",
    "  \t\tNone - just displays the image next to the bar chart  \n",
    "  \t'''\n",
    "\tresult_list = list(result_dict.items())\n",
    "\tresult_list = sorted(result_list, reverse=False, key=lambda result: result[1])\n",
    "\tcat_names = [x[0] for x in result_list]\n",
    "\tps = [x[1] for x in result_list]\t\n",
    "\tfig, (ax1, ax2) = plt.subplots(figsize=(9,12), ncols=2)\n",
    "\tax1.imshow(plt.imread(img_path))\n",
    "\tax1.axis('off')\n",
    "\t\n",
    "\t# create title:\n",
    "\ttitle = result_list[-1][0]\n",
    "\tfor i in range((len(result_list)-2), 0, -1):\n",
    "\t\tif result_list[i][1] > .5:\n",
    "\t\t\ttitle += (', ' + result_list[i][0])\n",
    "\tax1.set_title(title)\n",
    "\n",
    "\tax2.barh(range(len(cat_names)), ps, align='center')\n",
    "\tax2.set_aspect(0.1)\n",
    "\tax2.set_yticks(np.arange(len(cat_names)))\n",
    "\tax2.set_yticklabels(cat_names, size='small')\n",
    "\tax2.set_title('Class Probability')\n",
    "\tax2.set_xlim(0, 1.1)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\n",
    "\n",
    "# Get image and show it:\n",
    "img_dir = 'assets/DamageExamples'\n",
    "img_path = rand_img_path(img_dir)\n",
    "try:\n",
    "\timg = plt.imread(img_path)\n",
    "except:\n",
    "\tprint(\"ERROR: {} is no image file. Don't leave non-image files in the {} folder or change the img_dir!\".format(img_path, img_dir))\n",
    "plt.imshow(img)\n",
    "\n",
    "# Preprocess:\n",
    "img_proc = process_img(img_path)\n",
    "print(\"The datatype of the preprocessed image is: {} and its shape: {}\".format(img_proc.dtype, img_proc.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feed the dacl-model\n",
    "Finally, you can instantiate the model, load a checkpoint of your choice and feed the dacl-model with tasty preprocessed image data. You will see the predicted result under the following code cell.\n",
    "\n",
    "**Choose a checkpoint from the table below**\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Modelname             | Dataset           | EMR   | F1   | Tag          | Checkpoint                |\n",
    "|-----------------------|-------------------|-------|------|--------------|---------------------------|\n",
    "| Code_res_dacl         | codebrim_balanced | 73.73 | 0.85 | ResNet       | Code_res_dacl.pth         |\n",
    "| Code_mobilev2_dacl    | codebrim_balanced |70.41  | 0.84 | MobileNetV2  | Code_mobilev2_dacl.pth    |\n",
    "| Code_mobile_dacl      | codebrim_balanced | 69.46 | 0.83 | MobileNet    | Code_mobile_dacl.pth      |\n",
    "| Code_eff_dacl         | codebrim_balanced | 68.67 | 0.84 | EfficientNet | Code_eff_dacl.pth         |\n",
    "| McdsBikit_mobile_dacl | mcds_Bikit        | 54.44 | 0.66 | MobileNet    | McdsBikit_mobile_dacl.pth |\n",
    "| McdsBikit_eff_dacl    | mcds_Bikit        | 51.85 | 0.65 | EfficientNet | McdsBikit_eff_dacl.pth    |\n",
    "| McdsBikit_res_dacl    | mcds_Bikit        | 48.15 | 0.62 | ResNet       | McdsBikit_res_dacl.pth    |\n",
    "\n",
    "** *All these models are available via **bikit**. Check out this repo's README for further information!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which checkpoint/model you want to load from the table above:\n",
    "cp_name = 'Code_res_dacl.pth'\n",
    "\n",
    "# Load the checkpoint:\n",
    "cp = torch.load(Path('models/' + cp_name)) \n",
    "\n",
    "# Instantiate the model:\n",
    "model = DaclNet(base_name=cp['base'], resolution = cp['resolution'], hidden_layers=cp['hidden_layers'], \n",
    "\t\t\t\tdrop_prob=cp['drop_prob'], num_class=cp['num_class'])\n",
    "model.load_state_dict(cp['state_dict']) # Load the pre-trained weights into the model\n",
    "model.eval() # Set the model to eval-mode. No dropout and no autograd will be applied.\n",
    "\n",
    "# Now, let's feed the dacl-model in order to  classify the preprocessed image that we imported at the beginning:\n",
    "logits = model(img_proc)\n",
    "\n",
    "# Apply sigmoid activation to get predictions:\n",
    "preds = torch.sigmoid(logits).float().squeeze(0)\n",
    "\n",
    "# Binarize results:\n",
    "threshold = .5 # Which threshold do you want to choose for binarization of predictions? (for bikit, 0.5 was chosen)\n",
    "bin = np.array(preds > threshold, dtype=float)\n",
    "\n",
    "# In the cat_to_name file our damage-class-names are stored with the according position in the output vector:\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "\tcat_to_name = json.load(f)[cp['dataset']]\n",
    "\n",
    "# Output:\n",
    "# Make a dict with the predictions:\n",
    "preds_dict = {v:round(preds[int(k)].item(),2) for k,v in cat_to_name.items()}\n",
    "print('*'*10, 'Output', '*'*10)\n",
    "\n",
    "for k,v in preds_dict.items():\n",
    "\tif v > .5:\n",
    "\t\tprint('%s: %.2f%%' % (k,v*100)) \n",
    "\n",
    "# View the classified image and it's predictions:\n",
    "view_classify(img_path, preds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InProgress: Analyze the results\n",
    "Analyze the dacl-model's results and try to interpret them. Here is planned to provide code for testing on test datasets from bikit."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be5efd023f3affc850b9d9dfb74d7430ce06302eaa9619162d325c43dbb5c06e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
